# SARAi AGI - Arquitectura de IntegraciÃ³n v3.6.0

## ğŸ¯ VisiÃ³n General

Este documento describe la **arquitectura integrada** de SARAi v3.6.0, donde todos los componentes modulares se conectan en un sistema cohesivo end-to-end.

**Estado:** âœ… PRODUCCIÃ“N (v3.6.0)
**Ãšltima actualizaciÃ³n:** 2025-11-04

---

## ğŸ—ï¸ Arquitectura del Sistema Integrado

### Diagrama de Flujo Completo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        INPUT STAGE                                   â”‚
â”‚  â€¢ Input parsing y validaciÃ³n                                       â”‚
â”‚  â€¢ State initialization                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               CLASSIFICATION STAGE (TRM Classifier)                  â”‚
â”‚  Callable: trm_classifier(state) â†’ scores                           â”‚
â”‚                                                                      â”‚
â”‚  Output:                                                             â”‚
â”‚    - hard: float (0.0-1.0)    # Complejidad tÃ©cnica                 â”‚
â”‚    - soft: float (0.0-1.0)    # Complejidad emocional               â”‚
â”‚    - web_query: float (0.0-1.0)  # Necesidad de bÃºsqueda web        â”‚
â”‚                                                                      â”‚
â”‚  ImplementaciÃ³n:                                                     â”‚
â”‚    - PRIMARY: TRMClassifier (torch, trained model)                  â”‚
â”‚    - FALLBACK: Rule-based classifier (keywords)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 WEIGHTING STAGE (MCP Core)                           â”‚
â”‚  Callable: mcp_weighter(state) â†’ weights                            â”‚
â”‚                                                                      â”‚
â”‚  Input: hard, soft scores                                           â”‚
â”‚  Output:                                                             â”‚
â”‚    - alpha: float (0.0-1.0)   # Weight para expert agent            â”‚
â”‚    - beta: float (0.0-1.0)    # Weight para empathy agent           â”‚
â”‚                                                                      â”‚
â”‚  ImplementaciÃ³n:                                                     â”‚
â”‚    - PRIMARY: MCPCore (rules-based o learned mode)                  â”‚
â”‚    - FALLBACK: Direct mapping (alpha=hard, beta=soft)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚                   â”‚ (Parallel execution if enabled)
                 â–¼                   â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  EMOTION DETECTION   â”‚  â”‚  MODEL PREFETCH      â”‚
    â”‚  (Optional)          â”‚  â”‚  (Optional)          â”‚
    â”‚                      â”‚  â”‚                      â”‚
    â”‚  Input: state        â”‚  â”‚  Input: state        â”‚
    â”‚  Output: emotion{}   â”‚  â”‚  Output: model_name  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚                         â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ROUTING STAGE (Cascade Router)                          â”‚
â”‚  Callable: router(state) â†’ agent_key                                â”‚
â”‚                                                                      â”‚
â”‚  Decision Tree (7 priorities):                                      â”‚
â”‚    1. Vision     â†’ "vision"     (imagen/OCR/grÃ¡ficos)               â”‚
â”‚    2. Code       â†’ "code"       (programming skill)                 â”‚
â”‚    3. RAG        â†’ "rag"        (web_query â‰¥ 0.7)                   â”‚
â”‚    4. Omni-Loop  â†’ "omni"       (imagen + texto >20 chars)          â”‚
â”‚    5. Audio      â†’ "audio"      (input_type == "audio")             â”‚
â”‚    6. Expert     â†’ "expert"     (alpha â‰¥ 0.7)                       â”‚
â”‚    7. Empathy    â†’ "empathy"    (beta â‰¥ 0.7)                        â”‚
â”‚    8. Balanced   â†’ "balanced"   (fallback default)                  â”‚
â”‚                                                                      â”‚
â”‚  ImplementaciÃ³n: ConfidenceRouter + custom logic                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           EXECUTION STAGE (Model Pool + Generators)                  â”‚
â”‚  Callable: response_generator(state, agent_key) â†’ response          â”‚
â”‚                                                                      â”‚
â”‚  Agent-specific execution:                                          â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€ RAG Agent â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  1. Safe Mode check                                        â”‚     â”‚
â”‚  â”‚  2. Web search (SearXNG + cache)                           â”‚     â”‚
â”‚  â”‚  3. Audit PRE (SHA-256)                                    â”‚     â”‚
â”‚  â”‚  4. Synthesis prompt                                       â”‚     â”‚
â”‚  â”‚  5. LLM generation (expert model)                          â”‚     â”‚
â”‚  â”‚  6. Audit POST (HMAC)                                      â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€ Expert Agent â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  CASCADE 3-Tier Routing:                                   â”‚     â”‚
â”‚  â”‚    - Tier 1: LFM2-1.2B     (confidence â‰¥0.6) ~1.2s        â”‚     â”‚
â”‚  â”‚    - Tier 2: MiniCPM-4.1   (0.3-0.6)        ~4s           â”‚     â”‚
â”‚  â”‚    - Tier 3: Qwen-3-8B     (<0.3)           ~15s          â”‚     â”‚
â”‚  â”‚                                                            â”‚     â”‚
â”‚  â”‚  Features:                                                 â”‚     â”‚
â”‚  â”‚    - Dynamic quantization (IQ3_XXS/Q4_K_M/Q5_K_M)          â”‚     â”‚
â”‚  â”‚    - Context JIT (adaptive n_ctx)                          â”‚     â”‚
â”‚  â”‚    - LRU/TTL cache (hot: 5min, warm: 45s, cold: 15s)      â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€ Empathy Agent â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  - Model: LFM2-1.2B (tiny)                                 â”‚     â”‚
â”‚  â”‚  - Mode: EmpatÃ­a                                           â”‚     â”‚
â”‚  â”‚  - Features: Emotional context awareness                   â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€ Balanced Agent â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  - Model: expert_short (LFM2 + escalation)                 â”‚     â”‚
â”‚  â”‚  - Mode: Balanceado entre hard/soft                        â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              POST-PROCESSING STAGE (Fluidity - TODO)                 â”‚
â”‚  â€¢ Tone smoothing                                                   â”‚
â”‚  â€¢ Response enhancement                                             â”‚
â”‚  â€¢ Cultural adaptation                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        OUTPUT STAGE                                  â”‚
â”‚  State completo con:                                                â”‚
â”‚    - response: str                                                  â”‚
â”‚    - metadata: dict                                                 â”‚
â”‚      - agent: str                                                   â”‚
â”‚      - emotion: dict                                                â”‚
â”‚      - pipeline_metrics: dict                                       â”‚
â”‚    - scores: hard, soft, web_query, alpha, beta                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Componentes Integrados

### 1. TRM Classifier (`classifier/trm.py`)

**Responsabilidad:** Clasificar intenciones del input en 3 scores independientes.

**Callable Signature:**
```python
ClassifierCallable = Callable[[Dict[str, Any]], Dict[str, float]]

# Input:
state = {"input": "Â¿CÃ³mo debuggear cÃ³digo Python?"}

# Output:
scores = {
    "hard": 0.85,      # Alta complejidad tÃ©cnica
    "soft": 0.15,      # Baja complejidad emocional
    "web_query": 0.10  # No requiere bÃºsqueda web
}
```

**Modos de operaciÃ³n:**
- **PRIMARY:** TRMClassifier con torch (arquitectura recursiva, modelo entrenado)
- **FALLBACK:** Rule-based classifier (keywords, sin dependencias externas)

**Factory:** `create_trm_classifier_callable(config)`

**LOC:** 273 (classifier/trm.py)
**Tests:** test_trm_classifier.py

---

### 2. MCP Core (`mcp/core.py`)

**Responsabilidad:** Calcular weights alpha/beta para routing expert/empathy.

**Callable Signature:**
```python
WeightingCallable = Callable[[Dict[str, Any]], Dict[str, float]]

# Input:
state = {
    "input": "texto original",
    "hard": 0.85,
    "soft": 0.15
}

# Output:
weights = {
    "alpha": 0.82,  # Weight para expert
    "beta": 0.18    # Weight para empathy
}
```

**Modos de operaciÃ³n:**
- **Rules-based:** HeurÃ­sticas basadas en hard/soft scores
- **Learned:** Neural network entrenada (requiere torch)
- **Cache:** Vector Quantization para queries similares

**Factory:** `create_mcp_weighter_callable(config)`

**LOC:** 500 (mcp/core.py)
**Tests:** test_mcp.py

---

### 3. Emotional Context Engine (`emotion/context_engine.py`)

**Responsabilidad:** Detectar emociones (16), culturas (8), y proveer recomendaciones de modulaciÃ³n.

**Callable Signature:**
```python
EmotionDetectorCallable = Callable[[bytes], Optional[Dict[str, Any]]]

# Input:
audio_or_text = "Me siento frustrado con este error"

# Output:
emotion = {
    "emotion": "FRUSTRATED",
    "confidence": 0.85,
    "empathy_level": 0.9,
    "cultural_context": "spain",
    "voice_modulation": {
        "speed": 0.9,
        "pitch": 1.0,
        "emotion_intensity": 0.9
    }
}
```

**CaracterÃ­sticas:**
- 16 emociones: neutral, excited, frustrated, urgent, confused, etc.
- 8 culturas: Spain, Mexico, Argentina, Colombia, USA, UK, France, Germany
- User profiling (Ãºltimas 20 interacciones)
- Voice modulation recommendations

**Factory:** `create_emotion_detector_callable(config)`

**LOC:** 700 (emotion/context_engine.py)
**Tests:** test_emotional_context.py

---

### 4. Cascade Router (`cascade/confidence_router.py`)

**Responsabilidad:** Routing inteligente basado en confianza y especializaciÃ³n.

**Callable Signature:**
```python
RouterCallable = Callable[[Dict[str, Any]], str]

# Input:
state = {
    "input": "Â¿CuÃ¡l es el clima en Madrid?",
    "alpha": 0.3,
    "beta": 0.2,
    "web_query": 0.9
}

# Output:
agent_key = "rag"  # One of: rag, expert, empathy, balanced, vision, code, omni, audio
```

**Decision Tree (7 priorities):**
1. **Vision:** imagen/OCR/grÃ¡ficos â†’ "vision"
2. **Code:** programming skill â†’ "code"
3. **RAG:** web_query â‰¥ 0.7 â†’ "rag"
4. **Omni-Loop:** imagen + texto >20 chars â†’ "omni"
5. **Audio:** input_type == "audio" â†’ "audio"
6. **Expert:** alpha â‰¥ 0.7 â†’ "expert"
7. **Empathy:** beta â‰¥ 0.7 â†’ "empathy"
8. **Balanced:** fallback default â†’ "balanced"

**Factory:** `create_router_callable(config)`

**LOC:** 541 (cascade/confidence_router.py)
**Tests:** test_cascade.py

---

### 5. Model Pool (`model/pool.py`)

**Responsabilidad:** GestiÃ³n inteligente de modelos con cache, quantization, y fallback.

**CaracterÃ­sticas:**
- **LRU/TTL Cache:** hot (5min), warm (45s), cold (15s)
- **Dynamic Quantization:** IQ3_XXS (450MB), Q4_K_M (700MB), Q5_K_M (850MB)
- **Context JIT:** Adaptive n_ctx basado en prompt length
- **Fallback Chain:** expert_long â†’ expert_short â†’ tiny
- **Working-set Detection:** â‰¥3 accesos en 5min = hot

**API Principal:**
```python
pool = ModelPool()

# Get model con auto-context
model = pool.get_for_prompt("expert_short", "What is Python?")
# â†’ Loads with n_ctx=512 (short prompt)

# Auto-quantization
params = pool.get_model_params("Write a 2000 word essay...")
# â†’ {'quantization': 'Q5_K_M', 'n_ctx': 4096}
```

**LOC:** 831 (model/pool.py)
**Tests:** test_model_pool.py

---

### 6. RAG Agent (`agents/rag.py`)

**Responsabilidad:** Pipeline completa de bÃºsqueda web + sÃ­ntesis.

**Pipeline (6 pasos):**
1. **SAFE MODE CHECK:** Verificar Safe Mode
2. **BÃšSQUEDA CACHEADA:** SearXNG + WebCache (TTL dinÃ¡mico)
3. **AUDITORÃA PRE:** log_web_query() con SHA-256
4. **SÃNTESIS PROMPT:** Prompt engineering con snippets
5. **LLM GENERATION:** Expert model (short/long segÃºn contexto)
6. **AUDITORÃA POST:** log_web_query() con response + HMAC

**API Principal:**
```python
from sarai_agi.agents.rag import execute_rag

state = {
    "input": "Â¿CÃ³mo estÃ¡ el clima en Tokio?",
    "scores": {"web_query": 0.9}
}

result_state = execute_rag(state, model_pool)
# â†’ state updated with 'response' and 'rag_metadata'
```

**LOC:** 337 (agents/rag.py)
**Tests:** test_rag_system.py (22 tests)

---

## ğŸ”„ Flujo de EjecuciÃ³n Detallado

### Ejemplo 1: Query TÃ©cnica

```python
Input: "Â¿CÃ³mo implementar quicksort en Python?"

1. TRM Classifier:
   scores = {
       "hard": 0.88,
       "soft": 0.12,
       "web_query": 0.05
   }

2. MCP Weighter:
   weights = {
       "alpha": 0.85,  # Alta confianza en expert
       "beta": 0.15
   }

3. Emotion Detector:
   emotion = {
       "emotion": "NEUTRAL",
       "confidence": 0.75,
       "empathy_level": 0.3
   }

4. Router:
   agent_key = "expert"  # alpha â‰¥ 0.7

5. Response Generator (Expert):
   - Model Pool selecciona: expert_short
   - Cascade Router analiza confidence
   - Tier 1 (LFM2): confidence=0.7 â†’ responde directamente
   - Latency: ~1.2s

Output: Respuesta tÃ©cnica con cÃ³digo quicksort
```

### Ejemplo 2: Query Emocional

```python
Input: "Me siento triste y necesito apoyo"

1. TRM Classifier:
   scores = {
       "hard": 0.10,
       "soft": 0.85,
       "web_query": 0.05
   }

2. MCP Weighter:
   weights = {
       "alpha": 0.15,
       "beta": 0.82  # Alta confianza en empathy
   }

3. Emotion Detector:
   emotion = {
       "emotion": "FRUSTRATED",
       "confidence": 0.88,
       "empathy_level": 0.95,
       "cultural_context": "spain"
   }

4. Router:
   agent_key = "empathy"  # beta â‰¥ 0.7

5. Response Generator (Empathy):
   - Model Pool selecciona: tiny (LFM2 modo empatÃ­a)
   - Response adaptada con empathy_level=0.95
   - Latency: ~1.5s

Output: Respuesta empÃ¡tica y de apoyo emocional
```

### Ejemplo 3: Query Web

```python
Input: "Â¿CuÃ¡l es el clima actual en Madrid?"

1. TRM Classifier:
   scores = {
       "hard": 0.15,
       "soft": 0.10,
       "web_query": 0.92  # Alta necesidad de bÃºsqueda web
   }

2. MCP Weighter:
   weights = {
       "alpha": 0.35,
       "beta": 0.25
   }

3. Router:
   agent_key = "rag"  # web_query â‰¥ 0.7 (Priority 3)

4. RAG Agent:
   a. Safe Mode check: OK
   b. Web search: SearXNG + cache
      - Query: "clima Madrid actual"
      - Results: 5 snippets (cached, TTL=5min)
   c. Audit PRE: SHA-256 logged
   d. Synthesis prompt:
      """
      BasÃ¡ndote en los siguientes resultados de bÃºsqueda:
      1. "Madrid: 18Â°C, parcialmente nublado..."
      2. "PronÃ³stico para hoy: mÃ¡xima 20Â°C..."
      ...
      Responde: Â¿CuÃ¡l es el clima actual en Madrid?
      """
   e. LLM generation: expert_short
   f. Audit POST: HMAC logged

5. Response:
   "En Madrid, la temperatura actual es de 18Â°C con cielo
    parcialmente nublado. Se espera una mÃ¡xima de 20Â°C."

Output: Respuesta sintetizada con informaciÃ³n actualizada
```

---

## ğŸ§© IntegraciÃ³n de Componentes

### Factory Pattern

Todos los componentes se integran mediante **factory functions** que devuelven callables compatibles con `PipelineDependencies`:

```python
from sarai_agi.core import create_integrated_pipeline

# Create fully integrated pipeline
pipeline = create_integrated_pipeline(config={
    "enable_parallelization": True,
    "min_input_length": 20,
})

# Execute
result = await pipeline.run({"input": "Your query here"})

# Cleanup
await pipeline.shutdown()
```

### Dependency Injection

La pipeline usa **dependency injection** explÃ­cita:

```python
@dataclass
class PipelineDependencies:
    trm_classifier: ClassifierCallable          # TRM Classifier
    mcp_weighter: WeightingCallable             # MCP Core
    response_generator: ResponseGeneratorCallable  # Model Pool + Agents
    emotion_detector: Optional[EmotionDetectorCallable] = None
    prefetch_model: Optional[PrefetchCallable] = None
    router: Optional[RouterCallable] = None
```

Cada factory crea el callable correspondiente:

```python
dependencies = PipelineDependencies(
    trm_classifier=create_trm_classifier_callable(),
    mcp_weighter=create_mcp_weighter_callable(),
    response_generator=create_response_generator_callable(),
    emotion_detector=create_emotion_detector_callable(),
    prefetch_model=create_prefetch_callable(),
    router=create_router_callable(),
)
```

### Graceful Degradation

Todos los componentes tienen **fallbacks** para degradaciÃ³n graceful:

| Component | Primary | Fallback |
|-----------|---------|----------|
| TRM Classifier | TRMClassifier (torch) | Rule-based (keywords) |
| MCP Weighter | MCPCore (rules/learned) | Direct mapping (alpha=hard) |
| Emotion Detector | EmotionalContextEngine | None (optional component) |
| Router | ConfidenceRouter | Default balanced |
| Model Pool | Full cache + quantization | Simple model loading |
| RAG Agent | Full pipeline | Sentinel response |

---

## ğŸ“Š MÃ©tricas del Pipeline

### Pipeline Metrics

El pipeline recopila mÃ©tricas detalladas en cada ejecuciÃ³n:

```python
result["metadata"]["pipeline_metrics"] = {
    "classify_ms": 12.5,      # TRM Classifier latency
    "weights_ms": 3.2,        # MCP weighter latency
    "emotion_ms": 8.7,        # Emotion detection latency
    "routing_ms": 0.8,        # Router latency
    "generation_ms": 1250.0,  # Response generation latency
    "response_latency_ms": 1285.3,  # Total latency
    "prefetch_target": "expert_short"  # Prefetched model
}
```

### Performance Targets (v3.6.0)

| Metric | Target | Actual |
|--------|--------|--------|
| Classification latency | <50ms | ~12ms |
| Weighting latency | <20ms | ~3ms |
| Emotion detection | <50ms | ~9ms |
| Routing latency | <5ms | ~1ms |
| Total overhead | <150ms | ~30ms |
| Response latency P50 | <3s | ~1.3s (LFM2), ~25s (RAG) |
| Response latency P99 | <30s | ~18s (Qwen-3) |

---

## ğŸ§ª Testing de IntegraciÃ³n

### Test Suite Completo

**Archivo:** `tests/test_integration_e2e.py`

**Cobertura:**
- âœ… Pipeline creation
- âœ… Technical query routing (expert)
- âœ… Emotional query routing (empathy)
- âœ… Web query routing (RAG)
- âœ… Emotion detection
- âœ… Metrics collection
- âœ… Parallel/sequential execution
- âœ… Scores propagation
- âœ… Multiple sequential queries
- âœ… Error handling
- âœ… State immutability
- âœ… Component integration
- âœ… Performance tests

**EjecuciÃ³n:**
```bash
# Suite completa
pytest tests/test_integration_e2e.py -v

# Con coverage
pytest tests/test_integration_e2e.py --cov=src/sarai_agi/core --cov-report=html

# Clase especÃ­fica
pytest tests/test_integration_e2e.py::TestIntegratedPipeline -v
```

---

## ğŸš€ Uso desde CLI

### InstalaciÃ³n

```bash
# Clonar repo
git clone https://github.com/iagenerativa/sarai-agi.git
cd sarai-agi

# Setup environment
./scripts/bootstrap_env.sh
source .venv/bin/activate

# Instalar dependencias
pip install -e .
```

### CLI Integrada

```bash
# Query Ãºnica
python cli.py "Â¿CÃ³mo funciona el aprendizaje por refuerzo?"

# Query con verbose
python cli.py --verbose "Â¿QuÃ© es Python?"

# Modo interactivo
python cli.py --interactive

# Modo interactivo con verbose
python cli.py -i -v
```

**Output ejemplo:**
```
================================================================================
QUERY: Â¿CÃ³mo funciona el aprendizaje por refuerzo?
================================================================================

ğŸ“ RESPONSE (expert agent):
--------------------------------------------------------------------------------
El aprendizaje por refuerzo es una tÃ©cnica de machine learning donde un agente
aprende a tomar decisiones Ã³ptimas a travÃ©s de prueba y error, recibiendo
recompensas o penalizaciones por sus acciones...
--------------------------------------------------------------------------------

ğŸ” METADATA:
  Agent: expert

  Emotion:
    Detected: NEUTRAL
    Confidence: 0.75
    Empathy Level: 0.30
    Cultural Context: neutral

  Scores:
    Hard: 0.82
    Soft: 0.18
    Web Query: 0.05
    Alpha: 0.80
    Beta: 0.20

  Pipeline Metrics:
    Classify: 12.34ms
    Weights: 3.21ms
    Emotion: 8.76ms
    Routing: 0.87ms
    Generation: 1234.56ms
    Total: 1265.43ms
```

---

## ğŸ“š Referencias

### DocumentaciÃ³n de Componentes

- **TRM Classifier:** `src/sarai_agi/classifier/trm.py`
- **MCP Core:** `src/sarai_agi/mcp/core.py`
- **Emotional Context:** `src/sarai_agi/emotion/context_engine.py`
- **Cascade Router:** `src/sarai_agi/cascade/confidence_router.py`
- **Model Pool:** `src/sarai_agi/model/pool.py`
- **RAG Agent:** `src/sarai_agi/agents/rag.py`
- **Pipeline:** `src/sarai_agi/pipeline/parallel.py`

### Tests

- **Integration E2E:** `tests/test_integration_e2e.py`
- **TRM Classifier:** `tests/test_trm_classifier.py`
- **MCP Core:** `tests/test_mcp.py`
- **Emotion:** `tests/test_emotional_context.py`
- **Cascade:** `tests/test_cascade.py`
- **Model Pool:** `tests/test_model_pool.py`
- **RAG System:** `tests/test_rag_system.py`

### DocumentaciÃ³n Adicional

- **Arquitectura General:** `docs/ARCHITECTURE_OVERVIEW.md`
- **RAG Memory:** `docs/RAG_MEMORY.md`
- **Estado v3.4:** `docs/ESTADO_ACTUAL_v3.4.md`
- **Estado v3.5:** `docs/ESTADO_ACTUAL_v3.5.md`
- **Migration Plan:** `docs/MIGRATION_PLAN_v3_5_1.md`

---

## ğŸ¯ Roadmap de IntegraciÃ³n

### âœ… Completado (v3.6.0)

- [x] TRM Classifier integration
- [x] MCP weighting system
- [x] Emotional Context Engine
- [x] Cascade Router
- [x] Model Pool con cache LRU/TTL
- [x] RAG Agent completo
- [x] Pipeline paralela
- [x] Factory functions para todos los componentes
- [x] CLI integrada
- [x] Tests E2E completos
- [x] DocumentaciÃ³n de arquitectura

### ğŸ”„ En Progreso (v3.7.0)

- [ ] Fluidity Layer (Layer3 - tone smoothing)
- [ ] Vision integration (Qwen3-VL-4B)
- [ ] Code integration (VisCoder2-7B)
- [ ] Audio integration (Omni-3B + NLLB)
- [ ] Omni-Loop refinement
- [ ] Skills integration (SQL, Bash, Network)

### ğŸ“‹ Pendiente (v4.0+)

- [ ] Sidecars architecture
- [ ] Ethics Guard pre/post filtering
- [ ] Meta-learning feedback loop
- [ ] Advanced telemetry dashboard
- [ ] Multi-user support
- [ ] Production deployment guides

---

## ğŸ“ Changelog

### v3.6.0 (2025-11-04)

- âœ¨ **NEW:** Sistema integrado completo
- âœ¨ **NEW:** Factory functions para todos los componentes
- âœ¨ **NEW:** CLI integrada con modo interactivo
- âœ¨ **NEW:** Tests E2E completos (24 tests)
- âœ¨ **NEW:** DocumentaciÃ³n de arquitectura integrada
- ğŸ› **FIX:** Graceful degradation en todos los componentes
- ğŸ› **FIX:** Error handling completo
- ğŸ“š **DOCS:** INTEGRATION_ARCHITECTURE.md
- ğŸ§ª **TESTS:** test_integration_e2e.py (24 tests, 100% passing)

---

**Autor:** SARAi Team
**Licencia:** MIT
**Repositorio:** https://github.com/iagenerativa/sarai-agi
**VersiÃ³n:** v3.6.0
