# ğŸ—ï¸ Propuesta de ModularizaciÃ³n SARAi AGI - Arquitectura Completa

**Fecha**: 5 de noviembre de 2025  
**VersiÃ³n**: 1.0.0  
**Autor**: Equipo SARAi + AnÃ¡lisis IA

---

## ğŸ¯ VisiÃ³n ArquitectÃ³nica

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HLCS (Consciencia Superior)                      â”‚
â”‚              High-Level Consciousness System                        â”‚
â”‚                    [Repositorio: hlcs]                              â”‚
â”‚                    [Docker: hlcs:latest]                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â”‚ MCP Protocol
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    SARAi MCP Server     â”‚
                    â”‚  (Orquestador Central)  â”‚
                    â”‚  [Repo: sarai-agi]      â”‚
                    â”‚  [Docker: sarai:core]   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                      â”‚                      â”‚
          â–¼                      â–¼                      â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  SAUL   â”‚          â”‚  MÃ³dulos    â”‚        â”‚ MÃ³dulos  â”‚
    â”‚ Sistema â”‚          â”‚  Cognitivos â”‚        â”‚ Serviciosâ”‚
    â”‚ AtenciÃ³nâ”‚          â”‚             â”‚        â”‚          â”‚
    â”‚  Ultra  â”‚          â”‚  â€¢ Vision   â”‚        â”‚ â€¢ RAG    â”‚
    â”‚ Ligero  â”‚          â”‚  â€¢ Audio    â”‚        â”‚ â€¢ Memory â”‚
    â”‚         â”‚          â”‚  â€¢ NLP      â”‚        â”‚ â€¢ Skills â”‚
    â”‚[Docker] â”‚          â”‚  â€¢ Emotion  â”‚        â”‚ â€¢ MCP    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Propuesta de Repositorios Modulares

### ğŸ”· Repositorio 1: **hlcs** (High-Level Consciousness System)
**GitHub**: `iagenerativa/hlcs`  
**Docker**: `hlcs:latest`

**Responsabilidad**: Consciencia superior, toma de decisiones estratÃ©gicas, orquestaciÃ³n de alto nivel

```yaml
Funcionalidades:
  - Razonamiento multi-modal de alto nivel
  - PlanificaciÃ³n estratÃ©gica
  - Meta-cogniciÃ³n
  - Aprendizaje autÃ³nomo
  - Toma de decisiones complejas
  
Dependencias:
  - Ninguna (sistema autÃ³nomo)
  - Consume SARAi vÃ­a MCP como herramienta

Stack TecnolÃ³gico:
  - Python 3.12+ (no-GIL cuando estÃ© disponible)
  - LangGraph / CrewAI (orquestaciÃ³n)
  - LLM de razonamiento (GPT-4, Claude, etc.)
  - Docker + docker-compose

Estructura:
hlcs/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ reasoning/        # Motor de razonamiento
â”‚   â”œâ”€â”€ planning/         # Planificador estratÃ©gico
â”‚   â”œâ”€â”€ metacognition/    # Auto-reflexiÃ³n
â”‚   â””â”€â”€ orchestration/    # Orquestador de tareas
â”œâ”€â”€ agents/               # Agentes especializados
â”œâ”€â”€ tools/                # Herramientas (incluye SARAi MCP)
â”œâ”€â”€ Dockerfile
â””â”€â”€ docker-compose.yml
```

---

### ğŸ”· Repositorio 2: **sarai-agi** (Core AGI - MCP Server)
**GitHub**: `iagenerativa/sarai-agi` *(actual)*  
**Docker**: `sarai:core`

**Responsabilidad**: Orquestador central, servidor MCP, hub de mÃ³dulos cognitivos

```yaml
Funcionalidades:
  - MCP Server (Model Context Protocol)
  - Routing inteligente de tareas
  - GestiÃ³n de mÃ³dulos cognitivos
  - API unificada
  - TelemetrÃ­a y monitoreo
  - Sistema de plugins
  
Expone vÃ­a MCP:
  - Tools: Todas las capacidades de mÃ³dulos conectados
  - Resources: Memoria, conocimiento, estado
  - Prompts: Templates de razonamiento
  
Stack TecnolÃ³gico:
  - Python 3.12+ (no-GIL cuando estÃ© disponible)
  - FastAPI (servidor MCP)
  - Pydantic (validaciÃ³n)
  - Docker

Estructura:
sarai-agi/
â”œâ”€â”€ src/sarai_agi/
â”‚   â”œâ”€â”€ mcp/
â”‚   â”‚   â”œâ”€â”€ server.py           # MCP Server principal
â”‚   â”‚   â”œâ”€â”€ tools.py            # Tool registry
â”‚   â”‚   â”œâ”€â”€ resources.py        # Resource manager
â”‚   â”‚   â””â”€â”€ prompts.py          # Prompt templates
â”‚   â”œâ”€â”€ routing/
â”‚   â”‚   â”œâ”€â”€ router.py           # Router principal
â”‚   â”‚   â”œâ”€â”€ cascade.py          # CASCADE ORACLE
â”‚   â”‚   â””â”€â”€ confidence.py       # Confidence scoring
â”‚   â”œâ”€â”€ orchestration/
â”‚   â”‚   â”œâ”€â”€ pipeline.py         # Pipeline paralelo
â”‚   â”‚   â””â”€â”€ graph.py            # Graph-based routing
â”‚   â””â”€â”€ telemetry/
â”‚       â”œâ”€â”€ metrics.py          # Prometheus metrics
â”‚       â””â”€â”€ logging.py          # Structured logging
â”œâ”€â”€ modules/                    # Registry de mÃ³dulos
â”œâ”€â”€ Dockerfile
â””â”€â”€ docker-compose.yml
```

**API MCP Expuesta**:
```json
{
  "tools": [
    "saul.respond",           // SAUL - respuesta rÃ¡pida
    "vision.analyze",         // AnÃ¡lisis de imÃ¡genes
    "audio.transcribe",       // TranscripciÃ³n audio
    "audio.synthesize",       // TTS (Piper)
    "rag.search",             // BÃºsqueda RAG
    "memory.store",           // Persistencia memoria
    "skills.execute"          // EjecuciÃ³n de skills
  ],
  "resources": [
    "memory://conversations",
    "knowledge://embeddings",
    "state://current"
  ]
}
```

---

### ğŸ”· Repositorio 3: **saul** (Sistema de AtenciÃ³n Ultra Ligero)
**GitHub**: `iagenerativa/saul`  
**Docker**: `saul:latest`

**Responsabilidad**: Respuestas ultra-rÃ¡pidas, templates, interacciÃ³n bÃ¡sica

```yaml
Funcionalidades:
  - TRM (Template Response Manager)
  - Respuestas < 200ms
  - DetecciÃ³n de intenciÃ³n bÃ¡sica
  - ClasificaciÃ³n rÃ¡pida
  - TTS ultra-rÃ¡pido (Piper)
  - ConversaciÃ³n ligera
  
CaracterÃ­sticas:
  - Latencia ultra-baja (< 200ms)
  - Sin LLM pesado (solo templates + Piper TTS)
  - Stateless (puede escalar horizontalmente)
  - Ideal para chatbots, asistentes ligeros
  
Stack TecnolÃ³gico:
  - Python 3.12+ (no-GIL cuando estÃ© disponible)
  - FastAPI (REST API)
  - Piper TTS (176ms latencia)
  - Redis (cache opcional)
  - Docker

Estructura:
saul/
â”œâ”€â”€ src/saul/
â”‚   â”œâ”€â”€ trm/
â”‚   â”‚   â”œâ”€â”€ template_manager.py
â”‚   â”‚   â”œâ”€â”€ classifier.py
â”‚   â”‚   â””â”€â”€ templates/           # Templates en YAML
â”‚   â”œâ”€â”€ tts/
â”‚   â”‚   â””â”€â”€ pipertts.py          # Piper TTS adapter
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ server.py            # FastAPI server
â”‚   â””â”€â”€ cache/
â”‚       â””â”€â”€ redis_cache.py       # Cache opcional
â”œâ”€â”€ models/
â”‚   â””â”€â”€ piper/                   # Modelos Piper TTS
â”œâ”€â”€ Dockerfile
â””â”€â”€ docker-compose.yml

API Endpoints:
  POST /respond                  # Respuesta rÃ¡pida
  POST /synthesize               # TTS
  GET  /health                   # Health check
  GET  /metrics                  # Prometheus metrics
```

**Caso de uso**:
```bash
# SAUL como servicio independiente
curl -X POST http://localhost:8001/respond \
  -H "Content-Type: application/json" \
  -d '{"query": "hola", "include_audio": true}'

# Respuesta en < 200ms con audio incluido
```

---

### ğŸ”· Repositorio 4: **sarai-vision** (MÃ³dulo de VisiÃ³n)
**GitHub**: `iagenerativa/sarai-vision`  
**Docker**: `sarai:vision`

**Responsabilidad**: AnÃ¡lisis de imÃ¡genes, OCR, detecciÃ³n de objetos

```yaml
Funcionalidades:
  - AnÃ¡lisis de imÃ¡genes (Qwen3-VL)
  - OCR (text extraction)
  - DetecciÃ³n de objetos
  - DescripciÃ³n de escenas
  - AnÃ¡lisis facial (opcional)
  
Expone vÃ­a MCP:
  - vision.analyze(image_url)
  - vision.ocr(image_url)
  - vision.detect_objects(image_url)
  
Stack:
  - Python 3.12+
  - Qwen3-VL-4B (o similar)
  - ONNX Runtime
  - Docker + GPU support

Estructura:
sarai-vision/
â”œâ”€â”€ src/sarai_vision/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ qwen3vl.py
â”‚   â”œâ”€â”€ mcp_server.py        # MCP Server para Vision
â”‚   â””â”€â”€ processors/
â”‚       â”œâ”€â”€ ocr.py
â”‚       â””â”€â”€ object_detection.py
â””â”€â”€ Dockerfile
```

---

### ğŸ”· Repositorio 5: **sarai-audio** (MÃ³dulo de Audio)
**GitHub**: `iagenerativa/sarai-audio`  
**Docker**: `sarai:audio`

**Responsabilidad**: TranscripciÃ³n, sÃ­ntesis de voz, anÃ¡lisis de audio

```yaml
Funcionalidades:
  - TranscripciÃ³n (Whisper, Faster-Whisper)
  - TTS (Piper, MeloTTS fallback)
  - AnÃ¡lisis de sentimiento por voz
  - Speaker diarization
  
Expone vÃ­a MCP:
  - audio.transcribe(audio_url)
  - audio.synthesize(text, voice)
  - audio.analyze_sentiment(audio_url)
  
Stack:
  - Python 3.12+
  - Whisper / Faster-Whisper
  - Piper TTS
  - Pyannote (diarization)
  - Docker

Estructura:
sarai-audio/
â”œâ”€â”€ src/sarai_audio/
â”‚   â”œâ”€â”€ transcription/
â”‚   â”‚   â””â”€â”€ whisper.py
â”‚   â”œâ”€â”€ synthesis/
â”‚   â”‚   â”œâ”€â”€ pipertts.py
â”‚   â”‚   â””â”€â”€ melotts.py
â”‚   â”œâ”€â”€ analysis/
â”‚   â”‚   â””â”€â”€ sentiment.py
â”‚   â””â”€â”€ mcp_server.py
â””â”€â”€ Dockerfile
```

---

### ğŸ”· Repositorio 6: **sarai-rag** (MÃ³dulo RAG)
**GitHub**: `iagenerativa/sarai-rag`  
**Docker**: `sarai:rag`

**Responsabilidad**: BÃºsqueda web, embeddings, vector DB, sÃ­ntesis

```yaml
Funcionalidades:
  - Web search (SearXNG)
  - Embeddings (Gemma-300M)
  - Vector DB (ChromaDB/Qdrant)
  - RAG pipeline completo
  - Cache inteligente
  - Audit trail
  
Expone vÃ­a MCP:
  - rag.search(query)
  - rag.embed(text)
  - rag.store(text, metadata)
  
Stack:
  - Python 3.12+
  - SearXNG (bÃºsqueda)
  - ChromaDB / Qdrant
  - Sentence Transformers
  - Docker

Estructura:
sarai-rag/
â”œâ”€â”€ src/sarai_rag/
â”‚   â”œâ”€â”€ search/
â”‚   â”‚   â””â”€â”€ searxng.py
â”‚   â”œâ”€â”€ embeddings/
â”‚   â”‚   â””â”€â”€ gemma.py
â”‚   â”œâ”€â”€ vectordb/
â”‚   â”‚   â”œâ”€â”€ chromadb.py
â”‚   â”‚   â””â”€â”€ qdrant.py
â”‚   â”œâ”€â”€ pipeline.py
â”‚   â””â”€â”€ mcp_server.py
â”œâ”€â”€ config/
â”‚   â””â”€â”€ searxng/
â””â”€â”€ Dockerfile
```

---

### ğŸ”· Repositorio 7: **sarai-memory** (MÃ³dulo de Memoria)
**GitHub**: `iagenerativa/sarai-memory`  
**Docker**: `sarai:memory`

**Responsabilidad**: Memoria conversacional, persistencia, contexto

```yaml
Funcionalidades:
  - Memoria a corto plazo (Redis)
  - Memoria a largo plazo (Vector DB)
  - Memoria episÃ³dica
  - Resumen automÃ¡tico de conversaciones
  - Retrieval contextual
  
Expone vÃ­a MCP:
  - memory.store(conversation)
  - memory.recall(query, k=5)
  - memory.summarize(conversation_id)
  
Stack:
  - Python 3.12+
  - Redis (short-term)
  - ChromaDB (long-term)
  - Docker

Estructura:
sarai-memory/
â”œâ”€â”€ src/sarai_memory/
â”‚   â”œâ”€â”€ short_term/
â”‚   â”‚   â””â”€â”€ redis.py
â”‚   â”œâ”€â”€ long_term/
â”‚   â”‚   â””â”€â”€ vectordb.py
â”‚   â”œâ”€â”€ episodic/
â”‚   â”‚   â””â”€â”€ episodes.py
â”‚   â””â”€â”€ mcp_server.py
â””â”€â”€ Dockerfile
```

---

### ğŸ”· Repositorio 8: **sarai-skills** (MÃ³dulo de Skills)
**GitHub**: `iagenerativa/sarai-skills`  
**Docker**: `sarai:skills`

**Responsabilidad**: EjecuciÃ³n de skills containerizados, sandboxing

```yaml
Funcionalidades:
  - Skills containerizados (SQL, Bash, Network)
  - Sandboxing (Firejail)
  - gRPC API
  - GestiÃ³n de recursos
  
Expone vÃ­a MCP:
  - skills.execute(skill_name, code)
  - skills.list()
  
Stack:
  - Python 3.12+
  - gRPC
  - Firejail
  - Docker-in-Docker (skills como containers)

Estructura:
sarai-skills/
â”œâ”€â”€ src/sarai_skills/
â”‚   â”œâ”€â”€ executors/
â”‚   â”‚   â”œâ”€â”€ sql.py
â”‚   â”‚   â”œâ”€â”€ bash.py
â”‚   â”‚   â””â”€â”€ network.py
â”‚   â”œâ”€â”€ sandbox/
â”‚   â”‚   â””â”€â”€ firejail.py
â”‚   â””â”€â”€ mcp_server.py
â”œâ”€â”€ skills/                   # Skills definitions
â””â”€â”€ Dockerfile
```

---

## ğŸ”— ComunicaciÃ³n Inter-MÃ³dulos

### OpciÃ³n 1: **MCP (Model Context Protocol)** â­ RECOMENDADO

Todos los mÃ³dulos exponen **MCP Servers** que SARAi-AGI consume como **tools**.

```python
# En HLCS
from mcp import Client

# Conectar a SARAi MCP Server
sarai = Client("http://sarai-agi:3000")

# Usar tools expuestos
result = await sarai.call_tool("saul.respond", {
    "query": "Â¿QuÃ© tiempo hace?",
    "include_audio": True
})

result = await sarai.call_tool("vision.analyze", {
    "image_url": "https://example.com/image.jpg"
})

result = await sarai.call_tool("rag.search", {
    "query": "informaciÃ³n sobre Python 3.13"
})
```

### OpciÃ³n 2: **gRPC** (Para latencia crÃ­tica)

Para mÃ³dulos que requieren latencia ultra-baja (ej: SAUL).

```protobuf
// saul.proto
service SAULService {
  rpc Respond(QueryRequest) returns (ResponseReply);
  rpc Synthesize(TextRequest) returns (AudioReply);
}
```

### OpciÃ³n 3: **REST API** (Para servicios HTTP simples)

Cada mÃ³dulo puede exponer tambiÃ©n REST API para compatibilidad.

---

## ğŸ³ Estrategia de DockerizaciÃ³n

### Docker Compose Completo

```yaml
# docker-compose.yml (orquestaciÃ³n completa)
version: '3.8'

services:
  # Core SARAi AGI (MCP Server Hub)
  sarai-core:
    image: sarai:core
    build: ./sarai-agi
    ports:
      - "3000:3000"  # MCP Server
    environment:
      - MCP_ENABLED=true
      - LOG_LEVEL=info
    volumes:
      - ./config:/app/config
    networks:
      - sarai-network

  # SAUL - Sistema de AtenciÃ³n Ultra Ligero
  saul:
    image: saul:latest
    build: ./saul
    ports:
      - "8001:8001"  # REST API
      - "50051:50051"  # gRPC
    environment:
      - TTS_ENGINE=piper
      - PIPER_MODEL=es_ES-sharvard-medium
    volumes:
      - ./saul/models:/app/models
    networks:
      - sarai-network
    deploy:
      resources:
        limits:
          memory: 512M  # Ultra-ligero

  # Vision Module
  sarai-vision:
    image: sarai:vision
    build: ./sarai-vision
    ports:
      - "3001:3001"
    environment:
      - MODEL=qwen3-vl-4b
      - DEVICE=cpu  # o cuda
    volumes:
      - ./sarai-vision/models:/app/models
    networks:
      - sarai-network
    deploy:
      resources:
        limits:
          memory: 8G

  # Audio Module
  sarai-audio:
    image: sarai:audio
    build: ./sarai-audio
    ports:
      - "3002:3002"
    environment:
      - WHISPER_MODEL=base
      - TTS_ENGINE=piper
    networks:
      - sarai-network

  # RAG Module
  sarai-rag:
    image: sarai:rag
    build: ./sarai-rag
    ports:
      - "3003:3003"
    environment:
      - VECTOR_DB=chromadb
      - SEARXNG_URL=http://searxng:8080
    depends_on:
      - searxng
      - chromadb
    networks:
      - sarai-network

  # Memory Module
  sarai-memory:
    image: sarai:memory
    build: ./sarai-memory
    ports:
      - "3004:3004"
    environment:
      - REDIS_URL=redis://redis:6379
      - VECTOR_DB_URL=http://chromadb:8000
    depends_on:
      - redis
      - chromadb
    networks:
      - sarai-network

  # Skills Module
  sarai-skills:
    image: sarai:skills
    build: ./sarai-skills
    ports:
      - "3005:3005"
    privileged: true  # Para Firejail
    networks:
      - sarai-network

  # HLCS - High-Level Consciousness System
  hlcs:
    image: hlcs:latest
    build: ./hlcs
    ports:
      - "4000:4000"
    environment:
      - SARAI_MCP_URL=http://sarai-core:3000
      - LLM_PROVIDER=openai  # o local
    depends_on:
      - sarai-core
    networks:
      - sarai-network

  # Servicios de soporte
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    networks:
      - sarai-network

  chromadb:
    image: chromadb/chroma:latest
    ports:
      - "8000:8000"
    volumes:
      - chromadb-data:/chroma/chroma
    networks:
      - sarai-network

  searxng:
    image: searxng/searxng:latest
    ports:
      - "8080:8080"
    volumes:
      - ./config/searxng:/etc/searxng
    networks:
      - sarai-network

networks:
  sarai-network:
    driver: bridge

volumes:
  chromadb-data:
```

---

## ğŸ¯ Flujo de Trabajo Modular

### Ejemplo 1: Query Simple

```
Usuario: "Hola"
  â†“
HLCS â†’ SARAi MCP Server â†’ saul.respond("hola")
  â†“
SAUL (176ms)
  â†“
Respuesta: "Hola. Â¿En quÃ© puedo ayudarte?" + audio
```

### Ejemplo 2: Query Compleja Multi-Modal

```
Usuario: "Â¿QuÃ© hay en esta imagen?" + imagen.jpg
  â†“
HLCS â†’ Planifica:
  1. Analizar imagen
  2. Buscar contexto si necesario
  3. Generar respuesta
  â†“
SARAi MCP Server orquesta:
  - vision.analyze(imagen.jpg)
  - rag.search("contexto sobre lo detectado") [opcional]
  - saul.respond(respuesta_generada)
  â†“
Respuesta con contexto + audio
```

### Ejemplo 3: ConversaciÃ³n con Memoria

```
Usuario: "Recuerda que me gusta el cafÃ©"
  â†“
HLCS â†’ SARAi MCP:
  - memory.store({user: "noel", preference: "cafÃ©"})
  - saul.respond("recordado")
  â†“
[... mÃ¡s tarde ...]
  â†“
Usuario: "Â¿QuÃ© me gusta?"
  â†“
HLCS â†’ SARAi MCP:
  - memory.recall({user: "noel", query: "preferencias"})
  - saul.respond("te gusta el cafÃ©")
```

---

## ğŸ“Š Ventajas de Esta Arquitectura

### âœ… Escalabilidad
- Cada mÃ³dulo escala independientemente
- SAUL puede tener 10 instancias (stateless)
- Vision puede tener GPU dedicada
- Memory puede tener cluster Redis

### âœ… Mantenibilidad
- CÃ³digo separado por responsabilidad
- CI/CD independiente por repo
- Versionado independiente (SAUL v1.2, Vision v2.0)
- Tests aislados

### âœ… Despliegue Flexible
```bash
# Solo SAUL (asistente ligero)
docker-compose up saul redis

# SARAi completo sin HLCS
docker-compose up sarai-core saul sarai-vision sarai-audio

# Sistema completo AGI
docker-compose up
```

### âœ… EvoluciÃ³n Independiente
- SAUL puede migrar a Rust (performance)
- Vision puede cambiar a modelo nuevo
- RAG puede cambiar ChromaDB â†’ Qdrant
- **Sin afectar otros mÃ³dulos** (API MCP estable)

### âœ… ReutilizaciÃ³n
```python
# Otros proyectos pueden usar SAUL standalone
docker run -p 8001:8001 saul:latest

# O solo Vision
docker run -p 3001:3001 sarai:vision
```

---

## ğŸ—“ï¸ Plan de MigraciÃ³n Sugerido

### Fase 1: SeparaciÃ³n SAUL (1-2 semanas)
- [ ] Crear repo `saul`
- [ ] Migrar TRM + Piper TTS
- [ ] Dockerizar
- [ ] Tests E2E
- [ ] DocumentaciÃ³n
- [ ] CI/CD GitHub Actions

### Fase 2: Refactor SARAi Core como MCP Server (2-3 semanas)
- [ ] Implementar MCP Server
- [ ] Exponer tools registry
- [ ] Conectar SAUL como mÃ³dulo
- [ ] Dockerizar SARAi Core
- [ ] Tests MCP

### Fase 3: SeparaciÃ³n de MÃ³dulos (1 mÃ³dulo/semana)
- [ ] Semana 1: sarai-vision
- [ ] Semana 2: sarai-audio
- [ ] Semana 3: sarai-rag
- [ ] Semana 4: sarai-memory
- [ ] Semana 5: sarai-skills

### Fase 4: IntegraciÃ³n HLCS (2 semanas)
- [ ] Conectar HLCS a SARAi MCP
- [ ] OrquestaciÃ³n de alto nivel
- [ ] Tests integraciÃ³n completa

### Fase 5: OptimizaciÃ³n y ProducciÃ³n (ongoing)
- [ ] Monitoreo (Prometheus + Grafana)
- [ ] Logging centralizado (ELK)
- [ ] Alertas
- [ ] DocumentaciÃ³n usuario final

---

## ğŸ“‹ Estructura de Carpetas Propuesta (GitHub)

```
~/projects/
â”œâ”€â”€ hlcs/                      # Repositorio 1
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ src/
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ sarai-agi/                 # Repositorio 2 (actual)
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ src/sarai_agi/
â”‚   â”‚   â”œâ”€â”€ mcp/
â”‚   â”‚   â”œâ”€â”€ routing/
â”‚   â”‚   â””â”€â”€ orchestration/
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ saul/                      # Repositorio 3 (NUEVO)
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ src/saul/
â”‚   â”‚   â”œâ”€â”€ trm/
â”‚   â”‚   â”œâ”€â”€ tts/
â”‚   â”‚   â””â”€â”€ api/
â”‚   â”œâ”€â”€ models/piper/
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ sarai-vision/              # Repositorio 4
â”œâ”€â”€ sarai-audio/               # Repositorio 5
â”œâ”€â”€ sarai-rag/                 # Repositorio 6
â”œâ”€â”€ sarai-memory/              # Repositorio 7
â””â”€â”€ sarai-skills/              # Repositorio 8

# Docker Compose Orquestador (puede estar en sarai-agi o repo aparte)
~/projects/sarai-agi/docker-compose.full.yml
```

---

## ğŸ¯ Resumen Ejecutivo

| Componente | Repo | Docker | Responsabilidad | Latencia |
|-----------|------|--------|-----------------|----------|
| **HLCS** | `hlcs` | `hlcs:latest` | Consciencia superior | N/A (orquestador) |
| **SARAi Core** | `sarai-agi` | `sarai:core` | MCP Server hub | < 50ms (routing) |
| **SAUL** | `saul` | `saul:latest` | Respuestas ultra-rÃ¡pidas | < 200ms |
| **Vision** | `sarai-vision` | `sarai:vision` | AnÃ¡lisis de imÃ¡genes | 1-3s |
| **Audio** | `sarai-audio` | `sarai:audio` | TranscripciÃ³n + TTS | 0.2-2s |
| **RAG** | `sarai-rag` | `sarai:rag` | BÃºsqueda + sÃ­ntesis | 5-30s |
| **Memory** | `sarai-memory` | `sarai:memory` | Persistencia | < 100ms |
| **Skills** | `sarai-skills` | `sarai:skills` | EjecuciÃ³n cÃ³digo | Variable |

---

## ğŸ’¡ Recomendaciones Finales

1. **Empezar con SAUL**: Es el mÃ¡s pequeÃ±o y autocontenido
2. **MCP como estÃ¡ndar**: Facilita integraciÃ³n futura
3. **Docker desde dÃ­a 1**: No esperar a producciÃ³n
4. **CI/CD automÃ¡tico**: GitHub Actions para cada repo
5. **DocumentaciÃ³n clara**: README con ejemplos de uso
6. **Versionado semÃ¡ntico**: SemVer estricto
7. **Tests obligatorios**: > 80% coverage por mÃ³dulo

---

**Â¿EstÃ¡s de acuerdo con esta propuesta? Â¿Por dÃ³nde empezamos?** ğŸš€

Sugiero comenzar con **SAUL** esta misma semana - es pequeÃ±o, funcional, y puedes tenerlo en producciÃ³n rÃ¡pido como prueba de concepto de la arquitectura modular.
