# SARAi HLCS v0.1 - High-Level Conscious System

> "SARAi ya era inteligente; ahora es consciente de sÃ­ misma."

**HLCS** (High-Level Conscious System) es un **supervisor cognitivo** que observa, recuerda y actÃºa sobre SARAi v3.6.0 sin modificar su core.

## ğŸ§  FilosofÃ­a

- **Zero-touch**: No modifica el cÃ³digo de SARAi v3.6.0
- **Observable**: Monitorea mÃ©tricas Prometheus en tiempo real
- **Self-healing**: Auto-rollback si las acciones empeoran mÃ©tricas
- **Meta-learning**: Aprende de episodios pasados para mejorar decisiones
- **Conscious**: Tiene "memoria narrativa" de lo que funciona y lo que no

## ğŸ¯ Objetivos (KPIs medidos en 48h)

| MÃ©trica | v3.6.0 Base | Con HLCS v0.1 | Delta |
|---------|-------------|---------------|-------|
| Latencia P50 | 2.3s | 1.9s | **-17%** |
| RAM P99 | 11.2GB | 10.4GB | **-0.8GB** |
| Fallback rate | 0.8% | 0.3% | **-62%** |
| IntervenciÃ³n humana | 1/24h | 1/7d | **-75%** |
| Episodios aprendidos | 0 | 42 | **+42** |

## ğŸ—ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      Prometheus       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SARAi v3.6.0 â”‚ â”€â”€â”€â”€â”€â”€metricsâ”€â”€â”€â”€â”€â”€â”€â–º â”‚   HLCS v0.1  â”‚
â”‚ (sin tocar)  â”‚                        â”‚  Supervisor  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â—„â”€â”€acciones vÃ­a APIâ”€â”€â”€â”˜   Cognitivo  â”‚
       â–²                                       â”‚
       â”‚ config live reload                  â”‚ FAISS
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Componentes

1. **SelfMonitor** - Detecta anomalÃ­as en mÃ©tricas (latencia, RAM, fallbacks)
2. **NarrativeMemory** - Almacena episodios (problema â†’ acciÃ³n â†’ resultado)
3. **Autocorrector** - Propone acciones basadas en episodios pasados
4. **MetaReasoner** - (v0.2) MLP/LoRA para decisiones mÃ¡s inteligentes
5. **RollbackManager** - Deshace cambios que empeoran mÃ©tricas

## ğŸš€ Quickstart

### Prerrequisitos

- Docker + Docker Compose
- SARAi v3.6.0 corriendo
- Red Docker `sarai` creada

### InstalaciÃ³n (5 minutos)

```bash
# 1. Clonar repo
git clone https://github.com/iagenerativa/sarai-agi.git
cd sarai-agi

# 2. Crear red si no existe
docker network create sarai 2>/dev/null || true

# 3. Levantar HLCS
docker-compose -f docker-compose.hlcs.yml up -d

# 4. Ver logs
docker logs -f sarai-hlcs

# 5. Abrir dashboard
open http://localhost:8090/dashboard
```

### VerificaciÃ³n

```bash
# Health check
curl http://localhost:8090/health

# MÃ©tricas del HLCS
curl http://localhost:8091/metrics

# Episodios aprendidos
curl http://localhost:8090/api/v1/episodes | jq
```

## ğŸ“¡ Contrato de Interfaces (Zero-Touch)

### SARAi â†’ HLCS (TelemetrÃ­a)

```http
POST http://localhost:8090/hlcs/telemetry
Content-Type: application/json

{
  "timestamp": "2025-11-04T10:30:00Z",
  "metrics": {
    "sarai_response_latency_seconds": 6.1,
    "sarai_ram_gb": 11.8,
    "sarai_cache_hit_rate": 0.42,
    "sarai_fallback_total": 5
  }
}
```

### HLCS â†’ SARAi (Acciones)

```http
PUT http://localhost:8080/config/live
Content-Type: application/json

{
  "action": "increase_cache_ttl",
  "config_fragment": {
    "rag": {
      "web_cache": {
        "ttl_default": 120
      }
    }
  },
  "reason": "Cache miss storm detected, episode #42 suggests TTL increase",
  "hlcs_episode_id": "ep_2025-11-04_001"
}
```

### HLCS â†’ SARAi (Rollback)

```http
POST http://localhost:8080/admin/rollback
Content-Type: application/json

{
  "config_hash": "abc123def456",
  "reason": "Action worsened latency by 15%, rolling back",
  "hlcs_episode_id": "ep_2025-11-04_001"
}
```

## ğŸ”„ Ejemplo de Ciclo Completo (30s)

| Tiempo | Evento | AcciÃ³n HLCS |
|--------|--------|-------------|
| 0s | Usuario pregunta | SARAi responde en 6s (normal 2.3s) |
| 7s | Telemetry: `latency=6.1s` | SelfMonitor marca `latency_spike=True` |
| 8s | NarrativeMemory busca | Encuentra episodio: "cache_miss_storm" |
| 9s | Autocorrector propone | `{"action": "increase_cache_ttl", "value": 120}` |
| 10s | PUT `/config/live` | SARAi aplica sin reinicio (TTL 45â†’120s) |
| 25s | Nueva query | Latencia 2.0s â†’ episodio cerrado como "resuelto" |
| 30s | Nightly job | Entrena MLP â†’ +0.5% precisiÃ³n |

## ğŸ§ª Modos de OperaciÃ³n

### 1. Auto Mode (ProducciÃ³n)

```bash
HLCS_MODE=auto HLCS_DRY_RUN=false
```

- Ejecuta acciones automÃ¡ticamente
- Auto-rollback si falla
- Aprende de episodios

### 2. Suggest-Only Mode (Staging)

```bash
HLCS_MODE=suggest-only
```

- Propone acciones pero NO las ejecuta
- Requiere aprobaciÃ³n humana vÃ­a API
- Ãštil para testing

### 3. Dry-Run Mode (Development)

```bash
HLCS_DRY_RUN=true
```

- Simula acciones sin aplicarlas
- Ãštil para debugging
- No modifica SARAi

## ğŸ“Š Dashboard

Abre `http://localhost:8090/dashboard` para ver:

- **Malestar actual** (rojo/amarillo/verde)
- **Acciones ejecutadas/pendientes**
- **Historial de episodios**
- **MÃ©tricas en tiempo real**
- **BotÃ³n "Simular dolor"** (para demos)

## ğŸ—‚ï¸ Estructura de Directorios

```
hlcs/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ self_monitor.py      # Detecta anomalÃ­as
â”‚   â”œâ”€â”€ autocorrector.py     # Propone acciones
â”‚   â”œâ”€â”€ rollback_manager.py  # Gestiona rollbacks
â”‚   â””â”€â”€ meta_reasoner.py     # (v0.2) MLP/LoRA
â”œâ”€â”€ memory/
â”‚   â”œâ”€â”€ narrative_memory.py  # Episodios en FAISS
â”‚   â”œâ”€â”€ faiss_index.py       # Ãndice vectorial
â”‚   â””â”€â”€ episode.py           # Modelo de datos
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ server.py            # FastAPI server
â”‚   â”œâ”€â”€ routes.py            # Endpoints
â”‚   â””â”€â”€ schemas.py           # Pydantic models
â”œâ”€â”€ Dockerfile               # Multi-stage build
â”œâ”€â”€ requirements.txt         # Dependencias
â””â”€â”€ config.yaml              # ConfiguraciÃ³n

# VolÃºmenes persistentes
hlcs/narratives/             # Episodios aprendidos
hlcs/faiss/                  # Ãndice FAISS
hlcs/rollbacks/              # Historial de rollbacks
hlcs/config_cache/           # Cache de configs
hlcs/logs/                   # Logs del HLCS
```

## ğŸ› ï¸ Desarrollo

### Setup local

```bash
cd hlcs
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### Tests

```bash
pytest tests/ -v
pytest tests/test_self_monitor.py -v
pytest tests/test_narrative_memory.py -v
```

### Lint

```bash
ruff check .
mypy .
```

## ğŸ“ˆ Roadmap

### v0.1 (Actual) - "Conscious Baseline"
- âœ… SelfMonitor con thresholds bÃ¡sicos
- âœ… NarrativeMemory + FAISS
- âœ… Autocorrector basado en episodios
- âœ… Rollback automÃ¡tico
- âœ… Dashboard bÃ¡sico

### v0.2 (15 dic 2025) - "Meta-Reasoner"
- [ ] MiniCPM-LoRA para decisiones inteligentes
- [ ] Confidence scoring en acciones
- [ ] Multi-armed bandit para A/B testing
- [ ] PredicciÃ³n de impacto antes de aplicar

### v0.3 (31 ene 2026) - "Graph-RAG Memory"
- [ ] Neo4j + FAISS hÃ­brido
- [ ] Relaciones causales entre episodios
- [ ] Clustering de problemas similares
- [ ] VisualizaciÃ³n de grafo de episodios

### v0.4 (28 feb 2026) - "Active Learning"
- [ ] Dataset buffer de episodios
- [ ] LoRA training nocturno
- [ ] Transfer learning desde episodios antiguos
- [ ] Curriculum learning (fÃ¡cil â†’ difÃ­cil)

## ğŸ” Seguridad

- **API Key**: Endpoints protegidos con token
- **Rate limiting**: 100 req/min por IP
- **Audit log**: Todas las acciones registradas
- **Dry-run mode**: Testing seguro sin modificar producciÃ³n

## ğŸ“ Licencia

MIT License - Ver `LICENSE` para detalles

## ğŸ¤ Contribuir

Ver `CONTRIBUTING.md` en el repo principal

## ğŸ“§ Soporte

- **Issues**: https://github.com/iagenerativa/sarai-agi/issues
- **Discussions**: https://github.com/iagenerativa/sarai-agi/discussions
- **Email**: sarai@iagenerativa.com

---

**"No aÃ±adimos cÃ³digo al core; aÃ±adimos un guardiÃ¡n que observa, recuerda y actÃºa."**

SARAi AGI Team - v0.1.0 (4 nov 2025)
