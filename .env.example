
# ============================================================================
# LLM GATEWAY - Configuración Centralizada
# ============================================================================
# El LLM Gateway proporciona acceso unificado a múltiples providers de LLMs
# (Ollama, OpenAI, Anthropic, Local) con fallback automático, caching y métricas.
# 
# Beneficios:
# - Una sola instancia de Ollama compartida por todos los módulos (ahorra 4-8GB RAM)
# - Configuración centralizada en un solo lugar
# - Fallback automático si el provider primario falla
# - Cache de respuestas (reduce latencia y costos)
# - Métricas unificadas de uso de LLMs

# Provider principal (ollama | openai | anthropic | local)
LLM_GATEWAY_PRIMARY_PROVIDER=ollama

# Providers de fallback (separados por coma, en orden de preferencia)
LLM_GATEWAY_FALLBACK_PROVIDERS=local

# --------------------------------------------------------------------------
# OLLAMA CONFIGURATION
# --------------------------------------------------------------------------
LLM_GATEWAY_OLLAMA_BASE_URL=http://localhost:11434
LLM_GATEWAY_OLLAMA_MODEL=llama3.2:latest
LLM_GATEWAY_OLLAMA_TIMEOUT=300

# --------------------------------------------------------------------------
# OPENAI CONFIGURATION
# --------------------------------------------------------------------------
# LLM_GATEWAY_OPENAI_API_KEY=sk-...
LLM_GATEWAY_OPENAI_BASE_URL=https://api.openai.com/v1
LLM_GATEWAY_OPENAI_MODEL=gpt-4
LLM_GATEWAY_OPENAI_TIMEOUT=120

# --------------------------------------------------------------------------
# ANTHROPIC CONFIGURATION
# --------------------------------------------------------------------------
# LLM_GATEWAY_ANTHROPIC_API_KEY=sk-ant-...
LLM_GATEWAY_ANTHROPIC_BASE_URL=https://api.anthropic.com
LLM_GATEWAY_ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
LLM_GATEWAY_ANTHROPIC_TIMEOUT=120

# --------------------------------------------------------------------------
# LOCAL LLM CONFIGURATION (llama-cpp-python, LocalAI, etc.)
# --------------------------------------------------------------------------
LLM_GATEWAY_LOCAL_BASE_URL=http://localhost:8080
LLM_GATEWAY_LOCAL_MODEL=local-model
LLM_GATEWAY_LOCAL_TIMEOUT=300

# --------------------------------------------------------------------------
# CACHE CONFIGURATION
# --------------------------------------------------------------------------
LLM_GATEWAY_CACHE_ENABLED=true
LLM_GATEWAY_CACHE_TTL=3600        # Time-to-live en segundos (1 hora)
LLM_GATEWAY_CACHE_MAX_SIZE=1000   # Número máximo de respuestas

# --------------------------------------------------------------------------
# CONNECTION POOLING
# --------------------------------------------------------------------------
LLM_GATEWAY_POOL_SIZE=10
LLM_GATEWAY_POOL_TIMEOUT=30

# --------------------------------------------------------------------------
# RATE LIMITING
# --------------------------------------------------------------------------
LLM_GATEWAY_RATE_LIMIT_ENABLED=false
LLM_GATEWAY_RATE_LIMIT_RPM=60     # Requests per minute

# --------------------------------------------------------------------------
# MONITORING & LOGGING
# --------------------------------------------------------------------------
LLM_GATEWAY_METRICS_ENABLED=true
LLM_GATEWAY_LOG_LEVEL=INFO        # DEBUG | INFO | WARNING | ERROR
LLM_GATEWAY_LOG_REQUESTS=false    # Log cada request (verbose)

