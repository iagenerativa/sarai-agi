# Configuración por defecto para SARAi_AGI
# Estas opciones se ajustarán conforme se migren los módulos desde SARAi_v2.

version_base: "3.5.1"

pipeline:
  enable_parallelization: true
  min_input_length: 20
  max_workers: null  # auto-detectar

quantizacion:
  heuristicas:
    peso_tokens: 0.35
    peso_complejidad: 0.35
    peso_historial: 0.20
    peso_ram: 0.10
  calidades:
    IQ3_XXS:
      ram_mb: 450
      precision_estim: 0.78
    Q4_K_M:
      ram_mb: 760
      precision_estim: 0.92
    Q5_K_M:
      ram_mb: 980
      precision_estim: 0.96

telemetria:
  habilitada: true
  ventana_segundos: 30

seguridad:
  sanitizacion_inputs: true
  deteccion_amenazas: true

# ============================================================================
# LLM GATEWAY CONFIGURATION
# ============================================================================
# Gateway centralizado para acceso unificado a múltiples providers de LLMs
# Evita duplicación de instancias y centraliza la configuración

llm_gateway:
  # Provider principal a usar
  primary_provider: "ollama"  # ollama | openai | anthropic | local
  
  # Providers de fallback (en orden de preferencia)
  fallback_providers:
    - "local"
  
  # Configuración Ollama
  ollama:
    base_url: "http://localhost:11434"
    default_model: "llama3.2:latest"
    timeout: 300  # segundos
  
  # Configuración OpenAI (requiere API key)
  openai:
    base_url: "https://api.openai.com/v1"
    default_model: "gpt-4"
    api_key: null  # Set via LLM_GATEWAY_OPENAI_API_KEY env var
    timeout: 120
  
  # Configuración Anthropic (requiere API key)
  anthropic:
    base_url: "https://api.anthropic.com"
    default_model: "claude-3-5-sonnet-20241022"
    api_key: null  # Set via LLM_GATEWAY_ANTHROPIC_API_KEY env var
    timeout: 120
  
  # Configuración Local (llama-cpp-python, LocalAI, etc.)
  local:
    base_url: "http://localhost:8080"
    default_model: "local-model"
    timeout: 300
  
  # Cache de respuestas
  cache:
    enabled: true
    ttl: 3600  # Time-to-live en segundos (1 hora)
    max_size: 1000  # Número máximo de respuestas cacheadas
  
  # Connection pooling
  pool:
    size: 10
    timeout: 30
  
  # Rate limiting
  rate_limit:
    enabled: false
    requests_per_minute: 60
  
  # Monitoring
  metrics:
    enabled: true
  
  # Logging
  log:
    level: "INFO"  # DEBUG | INFO | WARNING | ERROR
    log_requests: false  # Log cada request (verbose)
