# SARAi v3.7.0 - Configuración Central
# Feature: MCP Server - Orquestador modular
# Hardware: CPU-only, 16GB RAM (12GB usables)
# Arquitectura: MCP Server + Módulos (SAUL, Vision, Audio, RAG, Memory, Skills)

runtime:
  backend: "cpu"  # "cpu" (GGUF) o "gpu" (4-bit) cuando migres
  cpu_model_format: "gguf"
  gpu_model_format: "4bit"
  max_concurrent_llms: 3  # v2.16: SOLAR (HTTP) + LFM2 + Omni-3B = 1.1GB (audio permanente)
  max_concurrent_skills: 3  # v2.12: Máximo de skills MoE simultáneos
  n_threads: 6  # os.cpu_count() - 2, deja núcleos libres (NEW v2.3)
  strategy: "hybrid_http_gguf"  # v2.16: HTTP para grandes, GGUF para pequeños

# NEW v3.7.0: MCP Server Configuration
mcp_server:
  enabled: true  # Feature flag para activar/desactivar MCP Server
  host: "0.0.0.0"  # Escuchar en todas las interfaces
  port: 3000  # Puerto por defecto (standard MCP)
  log_level: "info"  # debug, info, warning, error
  
  # Módulos a cargar
  modules:
    saul:
      enabled: true
      host: "localhost"  # Host del servidor SAUL gRPC
      port: 50051  # Puerto gRPC de SAUL
      timeout: 5.0  # Timeout en segundos
      fallback_mode: true  # Usar mock si SAUL no disponible
    
    # Futuros módulos (comentados hasta implementación)
    # vision:
    #   enabled: false
    #   host: "localhost"
    #   port: 3001
    # 
    # audio:
    #   enabled: false
    #   host: "localhost"
    #   port: 3002
    # 
    # rag:
    #   enabled: false
    #   host: "localhost"
    #   port: 3003
    # 
    # memory:
    #   enabled: false
    #   host: "localhost"
    #   port: 3004
    # 
    # skills:
    #   enabled: false
    #   host: "localhost"
    #   port: 3005

# NEW v3.5.1: Pipeline Paralelo - Reduce latencia 20% (-59ms)
pipeline:
  enable_parallel: true  # Feature flag para activar/desactivar
  max_workers: null  # null = auto-detect (cpu_count - 2), o especificar número
  bypass_threshold: 50  # Inputs <50 chars usan pipeline secuencial (overhead > ganancia)
  
memory:
  max_ram_gb: 12  # 4GB reservados para sistema
  model_ttl_seconds: 45  # Auto-descarga tras 45s (aumentado para prefetch)
  enable_swap: false  # NO usar swap, causa freezes
  use_mmap: true  # Mapeo de memoria para GGUF (NEW v2.3)
  use_mlock: false  # CRÍTICO: true puede causar OOM (NEW v2.3)

models:
  # Expert Tier: Razonamiento técnico profundo
  # NEW v2.16: Usa servidor Ollama HTTP (reduce RAM 11.6GB → 200MB)
  # Fallback: GGUF local si servidor no disponible
  expert:
    name: "SOLAR-10.7B-Instruct-v1.0"
    backend: "ollama_http"  # v2.16: HTTP primary
    fallback_backend: "gguf_native"  # v2.16: Fallback a GGUF
    repo_id: "upstage/SOLAR-10.7B-Instruct-v1.0"
    gguf_file: "SOLAR-10.7B-Instruct-v1.0-Q4_K_M.gguf"
    ollama_model_name: "hf.co/fblgit/UNA-SOLAR-10.7B-Instruct-v1.0:Q5_K_M"  # v2.16
    max_memory_mb: 200  # v2.16: Solo cliente HTTP (antes 6144 con GGUF)
    max_memory_mb_fallback: 6144  # v2.16: Si usa GGUF nativo
    cache_dir: "./models/cache/solar"
    context_length: 2048  # Default (expert_long), expert_short usa 512
    temperature: 0.7
    top_p: 0.9
    max_tokens: 1024
    
  # Tiny Tier: CONSOLIDADO (Lógica + RAG + Empatía) ⭐ TRIPLE FUNCIÓN
  # v2.16.1: LFM2 hace TODO, elimina necesidad de Qwen-1.7B (110 MB) y Omni LLM (1.4 GB)
  tiny:
    name: "LFM2-1.2B"
    repo_id: "LiquidAI/LFM2-1.2B"
    gguf_file: "LFM2-1.2B-Q4_K_M.gguf"
    model_path: "models/lfm2/LFM2-1.2B-Q4_K_M.gguf"  # NEW v3.3: Usar archivo local
    max_memory_mb: 700
    cache_dir: "./models/cache/lfm2"
    context_length: 2048
    permanent: true  # ✅ NUNCA DESCARGAR (usado constantemente)
    load_on_startup: true  # ✅ Cargar al inicio
    # Funciones consolidadas:
    #   1. LÓGICA: Procesa output del Audio-Encoder (text_es)
    #   2. RAG: Razonamiento con contexto recuperado (usa latent_z)
    #   3. EMPATÍA: Modulación emocional (usa emo_vec de audio)
    # Ahorro vs modelos separados:
    #   - Omni Text-LLM: 1.4 GB → 0 (eliminado)
    #   - Qwen-1.7B: 110 MB → 0 (eliminado)
    #   - LFM2 duplicado: 700 MB → 0 (un solo forward pass)
    #   TOTAL: 2.21 GB → 700 MB (ahorro 1.51 GB, -68%)
    temperature: 0.8
    top_p: 0.95
    max_tokens: 512
    
  # Embeddings: SIEMPRE en memoria
  embeddings:
    name: "EmbeddingGemma-300M"
    repo_id: "google/embeddinggemma-300m-qat-q4_0-unquantized"
    max_memory_mb: 150
    cache_dir: "./models/cache/embeddings"
    embedding_dim: 2048
    load_on_startup: true  # CRÍTICO: nunca descargar
    
  # ═══════════════════════════════════════════════════════════════════════════
  # BEST-OF-BREED v2.16.1 - Audio-Only + LFM2 Consolidado (ULTRA-OPTIMIZADO)
  # ═══════════════════════════════════════════════════════════════════════════
  # DESGLOSE TÉCNICO REAL:
  #
  # 1. AUDIO (190 MB ONNX):
  #    - Modelo: audio_only_q4.onnx (parte audio de Qwen3-VL-4B-Instruct)
  #    - Componentes: Audio-Encoder (80 MB) + Audio-Decoder (90 MB) + Projector (5 MB)
  #    - NO incluye: Text-LLM de 1.4 GB (usamos LFM2 en su lugar)
  #    - Función: STT español + TTS natural (MOS 4.21) + emotion detection
  #    - Latencia: <240ms (i7-1165G7)
  #
  # 2. TEXT LLM CONSOLIDADO ⭐ TRIPLE FUNCIÓN (700 MB GGUF):
  #    - Modelo: LFM2-1.2B-Q4_K_M.gguf
  #    - Funciones:
  #      1. LÓGICA: Procesa output STT (text_es) con razonamiento
  #      2. RAG: Recupera y razona con contexto (usa latent_z del audio)
  #      3. EMPATÍA: Modula respuesta con emoción (usa emo_vec)
  #    - Ventaja: UN modelo reemplaza 3 potenciales:
  #      • Omni Text-LLM: 1.4 GB → ELIMINADO
  #      • Qwen-1.7B (lógica): 110 MB → ELIMINADO
  #      • LFM2 duplicado (RAG): 700 MB → CONSOLIDADO en un solo forward pass
  #    - Ahorro total: 2.21 GB → 700 MB (-68%, ahorra 1.51 GB)
  #
  # 3. EXPERT (200 MB HTTP):
  #    - Modelo: SOLAR-10.7B via Ollama HTTP
  #    - Función: Razonamiento técnico profundo (alpha > 0.7)
  #
  # 4. VISIÓN (3.3 GB GGUF on-demand):
  #    - Modelo: Qwen3-VL-4B-Q6_K.gguf
  #    - Función: Análisis imagen/video
  #    - Carga: Bajo demanda, TTL 60s
  #
  # 5. TRADUCCIÓN (600 MB on-demand):
  #    - Modelo: NLLB-200-600M
  #    - Pipeline: Audio_X → STT → Translate(X→ES) → LFM2 → Translate(ES→X) → TTS_X
  #
  # RAM Profile ULTRA-OPTIMIZADO:
  #   BASELINE (Permanente): 1.29 GB (92% libre)
  #     - SOLAR HTTP:      0.2 GB
  #     - LFM2-1.2B (★):   0.7 GB   ← Lógica + RAG + Empatía
  #     - Audio ONNX:      0.19 GB  ← Solo STT/TTS/Emotion
  #     - EmbeddingGemma:  0.15 GB
  #     - TRM-Router:      0.05 GB
  #
  #   PEAK (Con visión + NLLB): ~5.5 GB (66% libre)
  #     Baseline + NLLB (0.6) + HiFi-GAN (0.035) + Qwen3-VL (3.3)
  #
  # Benchmarks Audio-Only (Qwen3-Omni-30B):
  #   - STT WER: ≤1.8% (español) - Esperado mejor que 3B (2.0%)
  #   - TTS MOS: ≥4.32 natural / ≥4.50 empatía - Esperado mejor que 3B (4.21/4.38)
  #   - Latencia audio: <240ms - Con INT8 + ONNX optimizado
  #   - Modelo: 30B parámetros (10x más grande que 3B)
  #   - PENDIENTE: Validación empírica con benchmarks reales
  #   - Latencia LFM2: 400ms (lógica + RAG + empatía en un pass)
  #   - Latencia E2E: 640ms (vs 1.6s con modelos separados, +60%)
  #   - RAM: 890 MB (vs 1.7-3.0 GB separados, -68%)
  #
  # Benchmarks Qwen3-VL-4B:
  #   - MMMU: 60.1%, MVBench: 71.9%, Video-MME: 65.8%
  #   - First-token: 500ms, RAM: 3.3 GB
  #
  # FILOSOFÍA: "Un componente, múltiples funciones = optimización máxima"
  # ═══════════════════════════════════════════════════════════════════════════
  
  # ═══════════════════════════════════════════════════════════════════════════
  # AUDIO AGENT v2.16.2 - MODULAR PIPELINE (OPTIMIZADO)
  # ═══════════════════════════════════════════════════════════════════════════
  # OPCIÓN 1: Pipeline Modular (RECOMENDADO)
  #   - Talker ONNX: qwen25_7b_audio.onnx (41MB, 4.29ms latencia) ⚡
  #   - Encoder: PyTorch Qwen2.5-Omni-7B (~3.5GB, 50-70ms)
  #   - Vocoder: PyTorch BigVGAN (~1.2GB, 30-40ms)
  #   - TOTAL: ~4.7GB RAM, ~100ms E2E (-30% vs monolítico)
  #
  # OPCIÓN 2: Pipeline Monolítico (FALLBACK)
  #   - Modelo: agi_audio_core_int8.onnx (1.1GB, ~140ms E2E)
  #   - Backward compatibility total
  #
  # Mejoras vs v2.16.1:
  #   ✅ Talker: 9-11x más rápido (40-50ms → 4.29ms)
  #   ✅ Tamaño: 96% reducción componente Talker (1.1GB → 41MB)
  #   ✅ Latencia E2E: 30% reducción proyectada (~140ms → ~100ms)
  #   ✅ Throughput: 10x mejora (11,654 tokens/s vs ~1,000 tok/s)
  #   ✅ Modularidad: Componentes separados, más mantenible
  # ═══════════════════════════════════════════════════════════════════════════
  audio_omni:
    name: "Qwen2.5-Omni-7B-Modular"
    
    # MODO DE PIPELINE ("modular" o "monolithic")
    pipeline_mode: "modular"  # v2.16.2: Usar pipeline optimizado
    
    # CONFIGURACIÓN MODULAR (v2.16.2)
    talker_path: "models/onnx/qwen25_7b_audio.onnx"  # 41MB optimizado ⚡
    encoder_backend: "pytorch"  # "pytorch" (recomendado) o "onnx"
    vocoder_backend: "pytorch"  # "pytorch" (recomendado) o "onnx"
    
    # CONFIGURACIÓN MONOLÍTICA (FALLBACK)
    model_type: "onnx"
    model_path: "models/onnx/agi_audio_core_int8.onnx"  # 1.1GB INT8
    backend: "onnxruntime"
    
    # CONFIGURACIÓN COMÚN
    max_memory_mb: 4700  # Modular: 3500 (Encoder) + 41 (Talker) + 1200 (Vocoder)
    permanent: true  # ✅ NUNCA DESCARGAR
    load_on_startup: true  # ✅ Cargar al inicio
    priority: "high"  # ✅ Prioridad de carga
    
    # Parámetros de audio
    sample_rate: 16000  # Qwen2.5-Omni usa 16kHz (no 22050)
    n_threads: 4
    temperature: 0.7
    
    # Benchmarks proyectados (modular):
    #   - Latencia Encoder: 50-70ms (PyTorch)
    #   - Latencia Talker: 4.29ms (ONNX optimizado) ⚡
    #   - Latencia Vocoder: 30-40ms (PyTorch)
    #   - Latencia E2E: ~100ms (-30% vs 140ms monolítico)
    #   - Throughput Talker: 11,654 tokens/s (10x mejora)
    #   - Real-time factor: 233x
    
  # VISION AGENT (BAJO DEMANDA)
  qwen3_vl_4b:
    name: "Qwen3-VL-4B"
    repo_id: "NexaAI/Qwen3-VL-4B-Instruct-GGUF"
    gguf_file: "Qwen3-VL-4B-Instruct.Q6_K.gguf"  # Q6_K = 6-bit optimizado
    backend: "gguf_native"
    max_memory_mb: 3300  # 4B Q6_K = ~3.3 GB RAM (mejor calidad vs Q4)
    cache_dir: "./models/cache/qwen3_vl"
    context_length: 4096  # Contexto estándar para visión
    n_ctx: 4096
    n_threads: 6
    permanent: false  # ❌ BAJO DEMANDA
    load_on_startup: false  # ❌ Solo cuando se necesite
    ttl_seconds: 60  # Auto-descarga tras 60s sin uso
    # Uso: Análisis de imagen/video, OCR, visual reasoning
    # Triggers: input_type in ["image", "video"]
    # Q6_K: Mayor precisión que Q4 (+0.5-1.0pp en benchmarks) con solo +500MB RAM
    temperature: 0.7
    max_tokens: 1024

  # ═══════════════════════════════════════════════════════════════════════════
  # NEW v2.12: MoE Skills - Modelos especializados bajo demanda
  # ═══════════════════════════════════════════════════════════════════════════
  # Skills Tier: Expertise especializada (carga bajo demanda)
  # - Máximo 3 skills simultáneos en RAM (config por LRU)
  # - Impacto RAM: ~800MB por skill (GGUF IQ4_NL, n_ctx=1024)
  # - TTL: 45 segundos sin uso → auto-descarga
  # - Fallback: Si skill falla, cae back a expert_short
  # ═══════════════════════════════════════════════════════════════════════════
  skills:
    # Skill 1: Programming (desarrollo de software)
    programming:
      name: "CodeLlama-7B-Instruct"
      repo_id: "TheBloke/CodeLlama-7B-Instruct-GGUF"
      gguf_file: "codellama-7b-instruct.Q4_K_M.gguf"
      max_memory_mb: 800
      cache_dir: "./models/cache/skills/programming"
      context_length: 1024
      temperature: 0.2  # Baja para código preciso
      max_tokens: 512
      domains: ["código", "programación", "python", "javascript", "debugging", "función", "clase", "algoritmo"]
    
    # Skill 2: Diagnosis (diagnóstico de sistemas)
    diagnosis:
      name: "Mistral-7B-Instruct"
      repo_id: "TheBloke/Mistral-7B-Instruct-v0.2-GGUF"
      gguf_file: "mistral-7b-instruct-v0.2.Q4_K_M.gguf"
      max_memory_mb: 800
      cache_dir: "./models/cache/skills/diagnosis"
      context_length: 1024
      temperature: 0.3  # Baja para diagnóstico preciso
      max_tokens: 512
      domains: ["diagnóstico", "error", "logs", "sistema", "servidor", "fallo", "problema", "debug"]
    
    # Skill 3: Finance (análisis financiero)
    finance:
      name: "FinGPT-7B"
      repo_id: "TheBloke/FinGPT-Instruct-7B-GGUF"
      gguf_file: "fingpt-instruct-7b.Q4_K_M.gguf"
      max_memory_mb: 800
      cache_dir: "./models/cache/skills/finance"
      context_length: 1024
      temperature: 0.4
      max_tokens: 512
      domains: ["finanzas", "inversión", "acciones", "mercado", "presupuesto", "roi", "análisis financiero"]
    
    # Skill 4: Logic (razonamiento lógico)
    logic:
      name: "Mistral-7B-Instruct"
      repo_id: "TheBloke/Mistral-7B-Instruct-v0.2-GGUF"
      gguf_file: "mistral-7b-instruct-v0.2.Q4_K_M.gguf"
      max_memory_mb: 800
      cache_dir: "./models/cache/skills/logic"
      context_length: 1024
      temperature: 0.3
      max_tokens: 512
      domains: ["lógica", "razonamiento", "deducción", "prueba", "teorema", "silogismo", "falacia"]
    
    # Skill 5: Creative (generación creativa)
    creative:
      name: "Nous-Hermes-2-Mistral-7B"
      repo_id: "TheBloke/Nous-Hermes-2-Mistral-7B-DPO-GGUF"
      gguf_file: "nous-hermes-2-mistral-7b-dpo.Q4_K_M.gguf"
      max_memory_mb: 800
      cache_dir: "./models/cache/skills/creative"
      context_length: 1024
      temperature: 0.9  # Alta para creatividad
      max_tokens: 512
      domains: ["historia", "cuento", "poema", "creativo", "narrativa", "personaje", "diálogo"]
    
    # Skill 6: Reasoning (razonamiento complejo)
    reasoning:
      name: "OpenHermes-2.5-Mistral-7B"
      repo_id: "TheBloke/OpenHermes-2.5-Mistral-7B-GGUF"
      gguf_file: "openhermes-2.5-mistral-7b.Q4_K_M.gguf"
      max_memory_mb: 800
      cache_dir: "./models/cache/skills/reasoning"
      context_length: 1024
      temperature: 0.5
      max_tokens: 512
      domains: ["razonamiento", "análisis", "paso a paso", "cadena de pensamiento", "explicar", "desglosar"]

# NEW v2.16: Configuración de Ollama HTTP
ollama:
  enabled: true
  base_url: "${OLLAMA_BASE_URL}"  # Lee de .env
  timeout: "${OLLAMA_TIMEOUT}"
  retries: "${OLLAMA_RETRIES}"
  prefer_http: true  # v2.16: Preferir HTTP sobre GGUF para SOLAR
  fallback_to_gguf: true  # v2.16: Usar GGUF si HTTP falla

# TRM-Router: Clasificador de intenciones
trm:
  base_model_path: "models/trm_base/trm_classifier.pt"
  skills_dir: "models/soft_skills"
  d_model: 256
  d_latent: 256
  h_cycles: 3  # Ciclos alto nivel
  l_cycles: 4  # Iteraciones por ciclo
  projection_dim: 256  # 2048 (embedding) → 256

# NEW v2.3: TRM-Mini para prefetching
trm_mini:
  model_path: "models/trm_mini/trm_mini.pt"
  d_model: 128  # Reducido vs TRM-Router (256 → 128)
  K_cycles: 2  # Reducido vs TRM-Router (3 → 2)
  hard_threshold: 0.65  # Threshold para prefetch de expert
  soft_threshold: 0.65  # Threshold para prefetch de tiny

# NEW v2.3: Prefetcher proactivo
prefetcher:
  enabled: true
  debounce_delay: 0.3  # 300ms sin input antes de clasificar
  min_input_length: 10  # Ignorar inputs muy cortos

# MCP: Meta Control Plane
mcp:
  state_path: "state/mcp_state.pkl"
  phase_1_threshold: 100    # Feedbacks para pasar a MLP
  phase_2_threshold: 2000   # Feedbacks para pasar a Transformer
  alpha_threshold_hard: 0.9  # α > 0.9 → solo SOLAR
  beta_threshold_soft: 0.9   # β > 0.9 → solo LFM2
  default_alpha: 0.6
  default_beta: 0.4
  
  # NEW v2.3: Fast-cache semántico
  cache_enabled: true
  cache_ttl: 60  # Segundos
  cache_quant_levels: 32  # Vector Quantization (5 bits)

# Feedback implícito
feedback:
  log_path: "logs/feedback_log.jsonl"
  async_processing: true  # Calcula embeddings en background
  timeout_seconds: 30  # Espera input_{t+1} por 30s
  
  # Umbrales de similitud semántica
  reformulation_threshold: 0.85  # input_t vs input_{t+1}
  confirmation_threshold: 0.7    # response vs input_{t+1}
  
  # Pesos de feedback
  negative_weight: -0.8  # Reformulación
  positive_weight: 0.9   # Confirmación
  neutral_weight: -0.3   # Timeout

# LangGraph: Orquestación
graph:
  state_schema:
    - input
    - hard
    - soft
    - alpha
    - beta
    - agent_used
    - hard_response
    - response
    - feedback
  
  checkpointer: "state/graph_checkpoints.pkl"
  max_iterations: 10

# Skills modulares (soft-skills + web_query v2.10)
skills:
  enabled:
    - empathy
    - creativity
    - web_query  # NEW v2.10
  
  empathy:
    trm_path: "models/soft_skills/empathy/trm.pt"
    threshold: 0.6
  
  creativity:
    trm_path: "models/soft_skills/creativity/trm.pt"
    threshold: 0.5
  
  # NEW v2.10: Skill RAG
  web_query:
    threshold: 0.7  # Activar solo si TRM-Router muy confiado
    priority: "normal"  # Nunca "critical" (respeta Fast Lane)

# NEW v2.10: Configuración RAG (Retrieval-Augmented Generation)
rag:
  enabled: true
  searxng_url: "http://localhost:8888"  # SearXNG local (docker)
  cache_dir: "state/web_cache"
  cache_ttl: 3600  # 1 hora (general)
  cache_ttl_time_sensitive: 300  # 5 minutos (clima, noticias, etc.)
  max_snippets: 5  # Limitar contexto LLM
  search_timeout: 10  # Timeout por búsqueda (segundos)
  
  # Auditoría web
  web_logs_dir: "logs"  # logs/web_queries_YYYY-MM-DD.jsonl
  enable_anomaly_detection: true
  webhook_url: ""  # Opcional: Slack/Discord para alertas
  
  # Síntesis LLM
  synthesis_max_tokens: 512
  synthesis_temperature: 0.3  # Bajo para síntesis factual
  synthesis_model_short_threshold: 1500  # chars de prompt

# Logging
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  file: "logs/sarai.log"
  max_bytes: 10485760  # 10MB
  backup_count: 5

# Testing
testing:
  max_ram_tolerance_gb: 12.5  # Permite 0.5GB overhead
  test_cases_path: "tests/fixtures/test_cases.json"

# ============================================================================
# NEW v2.11: Motor de Voz "Omni-Sentinel"
# ============================================================================
audio_engine:
  # Activación del motor de voz (omni3b o disabled)
  engine: "omni3b"  # Valores: "omni3b", "disabled"
  
  # Modelo ONNX
  model_path: "models/Qwen3-VL-4B-Instruct-es-q4.onnx"
  model_ram_mb: 2100  # ~2.1 GB en RAM
  
  # Configuración de audio
  sample_rate: 22050  # Hz
  audio_chunk_ms: 240  # Chunk mínimo para RT
  
  # Latencia target
  target_latency_ms: 250  # P50 objetivo
  
  # API REST
  port: 8001
  host: "0.0.0.0"
  
  # Logs HMAC
  logs_dir: "logs/audio"
  enable_hmac: true
  
  # Sentinel responses (voz)
  sentinel_responses:
    safe_mode:
      text: "Lo siento, SARAi está en modo seguro."
      emotion: "neutral"
      pitch_offset: 0.0
    
    model_load_failed:
      text: "No pude cargar el modelo de voz."
      emotion: "concerned"
      pitch_offset: -1.0

# ============================================================================
# NEW v2.11: Skills de Infraestructura
# ============================================================================
skills_infra:
  # Home Operations (Home Assistant)
  home_ops:
    enabled: true
    home_assistant_url: "${HOME_ASSISTANT_URL}"  # Definir en .env
    home_assistant_token: ""  # Long-lived access token (configurar en .env)
    timeout: 10  # segundos
    
    # Comandos críticos (requieren dry-run obligatorio)
    critical_commands:
      - "climate.set_temperature"
      - "lock.unlock"
      - "alarm_control_panel.disarm"
    
    # Auditoría
    logs_dir: "logs/skills/home_ops"
    enable_hmac: true
    
    # Sandbox
    use_firejail: true
    dry_run_by_default: true  # Seguridad primero
  
  # Network Diagnostics
  network_diag:
    enabled: true
    allowed_commands:
      - "ping"
      - "traceroute"
      - "speedtest"
    
    # Límites de seguridad
    max_ping_count: 5
    max_traceroute_hops: 15
    timeout: 30  # segundos
    
    # Auditoría
    logs_dir: "logs/skills/network_diag"
    enable_hmac: true
    
    # Sandbox
    use_firejail: true

# ============================================================================
# NEW v2.11: Seguridad y Auditoría
# ============================================================================
security:
  # Logs inmutables con chattr +a
  enable_chattr: true  # Requiere permisos root
  chattr_directories:
    - "logs/audio"
    - "logs/skills"
    - "logs/web_queries"
  
  # HMAC secret (configurar en .env)
  hmac_secret_env: "SARAI_HMAC_SECRET"
  
  # Cron de verificación de integridad
  integrity_check:
    enabled: true
    interval_hours: 1  # Cada hora
    alert_webhook: ""  # Slack/Discord (opcional)
  
  # Safe Mode triggers adicionales
  safe_mode_triggers:
    - "hmac_verification_failed"
    - "audio_log_corruption"
    - "home_ops_unauthorized_access"
    - "network_diag_suspicious_activity"
  
  # Contenedores read-only
  docker:
    read_only: true
    volumes_explicit: true
    network_internal: true
